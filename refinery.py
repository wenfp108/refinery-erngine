import os, json, base64, requests, importlib.util, sys
from datetime import datetime, timedelta, timezone
import pandas as pd
from supabase import create_client
from github import Github

# === ğŸ›¡ï¸ 1. æ ¸å¿ƒé…ç½® (é€šè¿‡ Secrets æ³¨å…¥) ===
# æ‚¨çš„ç§äººé‡‘åº“ IDï¼Œå¼•æ“åœ¨æ­¤å¤„æ‰§è¡Œâ€œè¾“å…¥/è¾“å‡ºâ€æ“ä½œ
PRIVATE_BANK_ID = "wenfp108/Central-Bank" 

GITHUB_TOKEN = os.environ.get("GH_PAT") 
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")

if not all([GITHUB_TOKEN, SUPABASE_URL, SUPABASE_KEY]):
    sys.exit("âŒ [å®¡è®¡å¼‚å¸¸] ç¯å¢ƒå˜é‡é…ç½®ç¼ºå¤±ï¼Œè¯·æ£€æŸ¥ GitHub Secretsã€‚")

# åˆå§‹åŒ–åŸºç¡€è®¾æ–½
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
gh_client = Github(GITHUB_TOKEN)
private_repo = gh_client.get_repo(PRIVATE_BANK_ID)

# === ğŸ§© 2. æ’ä»¶å‘ç°ç³»ç»Ÿ (é€šç”¨æ€§æ ¸å¿ƒ) ===

def get_all_processors():
    """åŠ¨æ€æ‰«æå¹¶åŠ è½½ ./processors/ ç›®å½•ä¸‹çš„æ‰€æœ‰å®¡è®¡æ’ä»¶"""
    procs = {}
    proc_dir = "./processors"
    if not os.path.exists(proc_dir): return procs
    
    for filename in os.listdir(proc_dir):
        if filename.endswith(".py") and not filename.startswith("__"):
            name = filename[:-3]
            try:
                spec = importlib.util.spec_from_file_location(f"mod_{name}", os.path.join(proc_dir, filename))
                mod = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(mod)
                procs[name] = {
                    "module": mod,
                    "table_name": getattr(mod, "TABLE_NAME", f"{name}_logs"),
                    "archive_folder": getattr(mod, "ARCHIVE_FOLDER", name)
                }
            except Exception as e:
                print(f"âš ï¸ [ç³»ç»Ÿè­¦å‘Š] æ— æ³•åŠ è½½æ’ä»¶ {name}: {e}")
    return procs

# === ğŸ”¥ 3. æƒ…æŠ¥å¯¹å†² (Top N æ¯å°æ—¶ç®€æŠ¥) ===

def generate_hot_reports(processors_config):
    """åŸºäºå„æ’ä»¶è‡ªå®šä¹‰ç®—æ³•ç”Ÿæˆæ¯å°æ—¶çƒ­é—¨å¿«æŠ¥å¹¶æ¨å›ç§äººåº“"""
    print("\nğŸ”¥ [æƒ…æŠ¥å¯¹å†²] æ­£åœ¨å¯åŠ¨æ¯å°æ—¶çƒ­é—¨æƒ…æŠ¥å®¡è®¡...")
    bj_now = datetime.now(timezone(timedelta(hours=8)))
    report_data = {
        "timestamp_bj": bj_now.isoformat(),
        "brief": {}
    }

    for name, config in processors_config.items():
        if hasattr(config["module"], "get_hot_items"):
            try:
                # å†³ç­–æ¡æ•°é€»è¾‘å·²ä¸‹æ”¾åˆ°æ’ä»¶å†…éƒ¨
                hot_items = config["module"].get_hot_items(supabase, config["table_name"])
                if hot_items:
                    report_data["brief"][name] = hot_items
                    print(f"âœ… {name}: æˆåŠŸå¯¹å†²å‡º {len(hot_items)} æ¡å…³é”®ä¿¡å·")
            except Exception as e:
                print(f"âš ï¸ {name} æå–çƒ­é—¨å¤±è´¥: {e}")

    if report_data["brief"]:
        date_tag = bj_now.strftime('%Y%m%d')
        hour_tag = bj_now.strftime('%H')
        target_path = f"reports/hourly/{date_tag}_{hour_tag}.json"
        content = json.dumps(report_data, ensure_ascii=False, indent=2)
        
        try:
            old = private_repo.get_contents(target_path)
            private_repo.update_file(old.path, f"ğŸ”¥ Intelligence Brief: {hour_tag}h", content, old.sha)
        except:
            private_repo.create_file(target_path, f"ğŸš€ New Intelligence Brief: {hour_tag}h", content)
        print(f"âœ¨ æ¯å°æ—¶å¯¹å†²å¿«æŠ¥å·²åŒæ­¥è‡³ç§äººåº“: {target_path}")

# === ğŸšœ 4. æ»šåŠ¨æ”¶å‰² (7å¤©å¼¹æ€§çª—å£å½’æ¡£) ===

def perform_grand_harvest(processors_config):
    """æ¸…ç† 7 å¤©å‰æ—§èµ„äº§ï¼Œå‹åˆ¶ä¸º Parquet å†·å­˜å‚¨"""
    # æ»‘åŠ¨çª—å£ï¼šæ¸…ç† 7 å¤©å‰ä¹‹å‰çš„è¿‡æœŸæ•°æ®
    cutoff_date = (datetime.now() - timedelta(days=7)).isoformat()
    date_tag = datetime.now().strftime('%Y%m%d')
    print(f"\nğŸšœ [æ»šåŠ¨æ”¶å‰²] å¯åŠ¨å·¡æ£€ã€‚ç›®æ ‡ï¼šæ—©äº {cutoff_date} çš„é™ˆå¹´èµ„äº§...")
    
    for name, config in processors_config.items():
        table = config["table_name"]
        try:
            # åˆ†æ‰¹æŸ¥è¯¢
            res = supabase.table(table).select("*").lt("bj_time", cutoff_date).limit(5000).execute()
            if not res.data:
                print(f"â„¹ï¸ {table}: æœªå‘ç°è¿‡æœŸæ•°æ® (å½“å‰æ‰€æœ‰æ•°æ®å‡ä¸º 7 å¤©å†…çƒ­èµ„äº§)ã€‚")
                continue
            
            print(f"ğŸ“¦ {table}: å‘ç° {len(res.data)} æ¡è¿‡æœŸæ•°æ®ï¼Œå¼€å§‹èµ„äº§å‹åˆ¶...")
            local_file = f"{table}_{date_tag}.parquet"
            pd.DataFrame(res.data).astype(str).to_parquet(local_file, index=False)
            
            target_path = f"archives/{config['archive_folder']}/{date_tag}.parquet"
            with open(local_file, "rb") as f: content = f.read()
            
            try:
                old = private_repo.get_contents(target_path)
                private_repo.update_file(old.path, f"ğŸ“¦ Archive Update: {date_tag}", content, old.sha)
            except:
                private_repo.create_file(target_path, f"ğŸ“¦ Archive New: {date_tag}", content)
            
            # ğŸ›¡ï¸ é˜²å¾¡å‹æ“ä½œï¼šç¡®è®¤ä¸Šä¼ æˆåŠŸåï¼Œåˆ†æ‰¹åˆ é™¤ (200æ¡ä¸€æ³¢) ä»¥é¿å¼€ SQL é™åˆ¶
            ids = [row['id'] for row in res.data]
            print(f"ğŸ—‘ï¸ æ­£åœ¨æ¸…ç©º SQL å†å²ç¼“å­˜...")
            for i in range(0, len(ids), 200):
                supabase.table(table).delete().in_("id", ids[i:i+200]).execute()
            
            if os.path.exists(local_file): os.remove(local_file)
            print(f"âœ… {table}: èµ„äº§å½’æ¡£æˆåŠŸã€‚")
        except Exception as e:
            print(f"âŒ {table} å½’æ¡£å¤±è´¥ (å®¡è®¡å®˜å¼‚å¸¸): {e}")

# === ğŸ¦ 5. æ¬è¿é€»è¾‘ (æŒ‡çº¹é˜²é‡ä¸ 24 å°æ—¶å›æº¯) ===

def process_and_upload(path, sha, config):
    """SHA æŒ‡çº¹æ£€æŸ¥ -> æ•°æ®å…¥åº“"""
    # å¹‚ç­‰æ€§æ£€æŸ¥ï¼šé˜²é‡æ˜¯é˜²å¾¡å‹æ¶æ„çš„çµé­‚
    check = supabase.table("processed_files").select("file_sha").eq("file_sha", sha).execute()
    if check.data: return False 

    try:
        content_file = private_repo.get_contents(path)
        raw_data = json.loads(base64.b64decode(content_file.content).decode('utf-8'))
        items = config["module"].process(raw_data, path)
        
        if items:
            # ğŸ›¡ï¸ æ ¸å¿ƒä¿®å¤ï¼š500æ¡ä¸€æ³¢æ’å…¥ï¼Œé˜²æ­¢ 1000 æ¡ API æŠ¥é”™
            for i in range(0, len(items), 500):
                supabase.table(config["table_name"]).insert(items[i : i+500]).execute()
            
            # è®°å½• SHA æŒ‡çº¹é”
            supabase.table("processed_files").upsert({"file_sha": sha, "file_path": path}).execute()
            return True
    except Exception as e:
        print(f"âš ï¸ æ–‡ä»¶ {path} è§£æå¼‚å¸¸: {e}")
    return False

def sync_bank_to_sql(processors_config, full_scan=False):
    """
    ã€åŒæ­¥æ ¸å¿ƒã€‘é‡‡ç”¨ 24 å°æ—¶å›æº¯çª—å£ï¼Œå¯¹å†²é‡‡é›†å»¶è¿Ÿé£é™©ã€‚
    """
    print(f"\nğŸ¦ [ä¸­å¤®é“¶è¡Œ] å¯åŠ¨å·¡æ£€æ¨¡å¼: {'å…¨é‡æ‰«æ' if full_scan else '24h é‡å æ‰«æ'}...")
    
    if full_scan:
        for name, config in processors_config.items():
            folder = config["archive_folder"]
            try:
                contents = private_repo.get_contents(folder)
                while contents:
                    file_content = contents.pop(0)
                    if file_content.type == "dir":
                        contents.extend(private_repo.get_contents(file_content.path))
                    elif file_content.name.endswith(".json"):
                        process_and_upload(file_content.path, file_content.sha, config)
            except Exception as e:
                print(f"âš ï¸ æ‰«æç§äººåº“ {folder} å¤±è´¥: {e}")
    else:
        # å¯¹å†²å»¶è¿Ÿç­–ç•¥ï¼šå›æº¯è¿‡å» 24 å°æ—¶çš„æ‰€æœ‰ Commit
        since = datetime.now(timezone.utc) - timedelta(hours=24)
        commits = private_repo.get_commits(since=since)
        for commit in commits:
            for f in commit.files:
                if f.filename.endswith('.json'):
                    source_key = f.filename.split('/')[0]
                    if source_key in processors_config:
                        process_and_upload(f.filename, f.sha, processors_config[source_key])

# === ğŸš€ 6. æ‰§è¡Œå…¥å£ ===

if __name__ == "__main__":
    # ç¯å¢ƒå‚æ•°æ§åˆ¶ï¼šæ”¯æŒæ‰‹åŠ¨å¼ºåˆ¶æ‰§è¡Œ
    IS_FULL_SCAN = os.environ.get("FORCE_FULL_SCAN", "false").lower() == "true"
    FORCE_HARVEST = os.environ.get("FORCE_HARVEST", "false").lower() == "true"

    # 1. ç¬¬ä¸€æ­¥ï¼šæ’ä»¶è£…è½½
    all_procs = get_all_processors()
    print(f"ğŸ” æ£€æµ‹åˆ° {len(all_procs)} ä¸ªæ´»è·ƒå¤„ç†å™¨æ’ä»¶")
    
    # 2. ç¬¬äºŒæ­¥ï¼šå¢é‡åŒæ­¥ (åŸºäº SHA æŒ‡çº¹é˜²é‡)
    sync_bank_to_sql(all_procs, full_scan=IS_FULL_SCAN)
    
    # 3. ç¬¬ä¸‰æ­¥ï¼šæƒ…æŠ¥æ—¶æŠ¥ç”Ÿæˆ (Top N)
    generate_hot_reports(all_procs)
    
    # 4. ç¬¬å››æ­¥ï¼šèµ„äº§æ”¶å‰² (å¼¹æ€§çª—å£ï¼šåŒ—äº¬æ—¶é—´å‡Œæ™¨ 4:00 - 6:00)
    # UTC 20ç‚¹ - 22ç‚¹ å‡ä¸ºæœ‰æ•ˆæ”¶å‰²æ—¶é—´ï¼Œå¯¹å†² GitHub Action å»¶è¿Ÿé£é™©
    current_hour_utc = datetime.now(timezone.utc).hour
    is_harvest_window = 20 <= current_hour_utc <= 22 

    if is_harvest_window or FORCE_HARVEST:
        perform_grand_harvest(all_procs)
    else:
        print(f"â³ å½“å‰ UTC æ—¶é—´ {current_hour_utc}hï¼Œæœªåˆ°é¢„å®šæ”¶å‰²æ—¶é—´ (20-22h UTC)ã€‚")
        
    print(f"\n[{datetime.now().strftime('%H:%M:%S')}] âœ… å®¡è®¡ä»»åŠ¡ç»“æŸã€‚")
