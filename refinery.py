import os, json, base64, requests, importlib.util, sys
from datetime import datetime, timedelta, timezone
import pandas as pd
from supabase import create_client
from github import Github

# === ğŸ›¡ï¸ 1. æ ¸å¿ƒé…ç½® ===
PRIVATE_BANK_ID = "wenfp108/Central-Bank" 
GITHUB_TOKEN = os.environ.get("GH_PAT") 
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")

if not all([GITHUB_TOKEN, SUPABASE_URL, SUPABASE_KEY]):
    sys.exit("âŒ [å®¡è®¡å¼‚å¸¸] ç¯å¢ƒå˜é‡ç¼ºå¤±ã€‚")

supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
gh_client = Github(GITHUB_TOKEN)
private_repo = gh_client.get_repo(PRIVATE_BANK_ID)

# === ğŸ§© 2. æ’ä»¶å‘ç°ç³»ç»Ÿ (ä¿æŒåŸæ ·) ===
def get_all_processors():
    procs = {}
    proc_dir = "./processors"
    if not os.path.exists(proc_dir): return procs
    for filename in os.listdir(proc_dir):
        if filename.endswith(".py") and not filename.startswith("__"):
            name = filename[:-3]
            try:
                spec = importlib.util.spec_from_file_location(f"mod_{name}", os.path.join(proc_dir, filename))
                mod = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(mod)
                procs[name] = {
                    "module": mod,
                    "table_name": getattr(mod, "TABLE_NAME", f"{name}_logs"),
                    "archive_folder": getattr(mod, "ARCHIVE_FOLDER", name)
                }
            except Exception as e: print(f"âš ï¸ æ’ä»¶ {name} åŠ è½½å¤±è´¥: {e}")
    return procs

# === ğŸ”¥ 3. æˆ˜æŠ¥å·¥å‚ï¼šMarkdown å‚ç›´å †å å¼•æ“ ===

def generate_hot_reports(processors_config):
    """
    ä¸åˆ©ç”¨ AIï¼Œç›´æ¥åŸºäº Alpha-Signal-V1 ç®—æ³•ç”Ÿæˆ MD æˆ˜æŠ¥ã€‚
    """
    print("\nğŸ”¥ [æƒ…æŠ¥å¯¹å†²] æ­£åœ¨ç”Ÿæˆå…¨ç»´åº¦ Markdown æ—¶æŠ¥...")
    bj_now = datetime.now(timezone(timedelta(hours=8)))
    date_tag = bj_now.strftime('%Y%m%d')
    hour_tag = bj_now.strftime('%H')
    
    # å»ºç«‹æˆ˜æŠ¥é¡µçœ‰
    md_report = f"# ğŸš€ Architect's Alpha æƒ…æŠ¥å®¡è®¡ ({date_tag} {hour_tag}:00)\n\n"
    md_report += "> **é˜²å¾¡çŠ¶æ€**ï¼šæ‰¿è®¤è¿æ°”å·® / ä¸¥ç¦æ æ† / ä¸“æ³¨æ•°æ®åŠ¨èƒ½\n\n"

    # éå†æ‰€æœ‰æ’ä»¶ï¼ŒæŒ‰æºå †å 
    for source_name, config in processors_config.items():
        if hasattr(config["module"], "get_hot_items"):
            try:
                # è·å–è¯¥æºçš„åˆ†æ¿å—å®¡è®¡çŸ©é˜µ
                sector_matrix = config["module"].get_hot_items(supabase, config["table_name"])
                if not sector_matrix: continue

                md_report += f"## ğŸ“¡ æ¥æºï¼š{source_name.upper()}\n"
                
                for sector, items in sector_matrix.items():
                    md_report += f"### ğŸ·ï¸ æ¿å—ï¼š{sector}\n"
                    md_report += "| ä¿¡å·å¼ºåº¦ | æºå¤´ | å…³é”®æƒ…æŠ¥æ‘˜è¦ | é“¾æ¥ |\n| :--- | :--- | :--- | :--- |\n"
                    
                    for item in items:
                        # æ‘˜è¦å¤„ç†ï¼šå¯¹é½åŒ—äº¬æ—¶é—´äº¤å‰å¯¹æ¯”
                        score = int(item.get('score', 0))
                        source = item.get('user_name', 'Unknown')
                        text = item.get('full_text', '').replace('\n', ' ')[:85] + "..."
                        url = item.get('tweet_url', '#')
                        md_report += f"| **{score:,}** | {source} | {text} | [æŸ¥çœ‹]({url}) |\n"
                    md_report += "\n"
            except Exception as e:
                print(f"âš ï¸ {source_name} æˆ˜æŠ¥æ¸²æŸ“å¤±è´¥: {e}")

    # åŒè·¯åŒæ­¥ï¼šlatest_brief.md (ç§’å¼€) + å†å²å½’æ¡£
    latest_path = "reports/latest_brief.md"
    archive_path = f"reports/hourly/{date_tag}_{hour_tag}.md"
    
    for path in [latest_path, archive_path]:
        try:
            try:
                old = private_repo.get_contents(path)
                private_repo.update_file(old.path, f"ğŸ“Š Update Brief: {hour_tag}h", md_report, old.sha)
            except:
                private_repo.create_file(path, f"ğŸš€ New Brief: {hour_tag}h", md_report)
        except Exception as e: print(f"âŒ å†™å…¥ {path} å¤±è´¥: {e}")

# === ğŸšœ 4. æ»šåŠ¨æ”¶å‰²ï¼šå« 7 å¤©æŠ¥è¡¨æ¸…ç† ===

def perform_grand_harvest(processors_config):
    """å‹åˆ¶æ—§èµ„äº§å¹¶æ¸…ç† 7 å¤©å‰æ—§æŠ¥è¡¨"""
    cutoff_date = (datetime.now() - timedelta(days=7))
    cutoff_str = cutoff_date.isoformat()
    print(f"\nğŸšœ [æ»šåŠ¨æ”¶å‰²] æ¸…ç†æ—©äº {cutoff_str} çš„èµ„äº§ä¸æŠ¥è¡¨...")

    # A. æ¸…ç† 7 å¤©å‰çš„ MD å†å²æŠ¥è¡¨
    try:
        all_reports = private_repo.get_contents("reports/hourly")
        for report in all_reports:
            # ç®€å•é€šè¿‡æ–‡ä»¶åæ—¥æœŸåˆ¤æ–­: 20260120_12.md
            if report.name.endswith(".md") and report.name[:8] < cutoff_date.strftime('%Y%m%d'):
                private_repo.delete_file(report.path, "ğŸ—‘ï¸ Cleanup old report", report.sha)
                print(f"ğŸ—‘ï¸ å·²æ¸…ç†è¿‡æœŸæŠ¥è¡¨: {report.name}")
    except: pass

    # B. SQL æ•°æ®å½’æ¡£ (ä¿æŒåŸé€»è¾‘)
    for name, config in processors_config.items():
        table = config["table_name"]
        try:
            res = supabase.table(table).select("*").lt("bj_time", cutoff_str).limit(5000).execute()
            if res.data:
                # ...æ­¤å¤„ä¿ç•™ä½ åŸæœ‰çš„ Parquet å‹åˆ¶ä¸ SQL åˆ é™¤é€»è¾‘...
                pass 
        except Exception as e: print(f"âŒ {table} å½’æ¡£å¤±è´¥: {e}")

# === ğŸ¦ 5. æ¬è¿é€»è¾‘ (é˜²é‡ä¸æ’å…¥é€»è¾‘ï¼Œä¿æŒåŸæ ·) ===
def process_and_upload(path, sha, config):
    check = supabase.table("processed_files").select("file_sha").eq("file_sha", sha).execute()
    if check.data: return False 
    try:
        content_file = private_repo.get_contents(path)
        raw_data = json.loads(base64.b64decode(content_file.content).decode('utf-8'))
        items = config["module"].process(raw_data, path)
        if items:
            for i in range(0, len(items), 500):
                supabase.table(config["table_name"]).insert(items[i : i+500]).execute()
            supabase.table("processed_files").upsert({"file_sha": sha, "file_path": path}).execute()
            return True
    except Exception as e: print(f"âš ï¸ {path} è§£æå¼‚å¸¸: {e}")
    return False

def sync_bank_to_sql(processors_config, full_scan=False):
    since = datetime.now(timezone.utc) - timedelta(hours=24)
    commits = private_repo.get_commits(since=since)
    for commit in commits:
        for f in commit.files:
            if f.filename.endswith('.json'):
                source_key = f.filename.split('/')[0]
                if source_key in processors_config:
                    process_and_upload(f.filename, f.sha, processors_config[source_key])

# === ğŸš€ 6. æ‰§è¡Œå…¥å£ ===
if __name__ == "__main__":
    all_procs = get_all_processors()
    sync_bank_to_sql(all_procs, full_scan=(os.environ.get("FORCE_FULL_SCAN")=="true"))
    generate_hot_reports(all_procs) # æ ¸å¿ƒä¿®æ”¹ï¼šæ¯å°æ—¶ç”Ÿæˆ MD æˆ˜æŠ¥
    
    # å‡Œæ™¨æ”¶å‰²çª—å£
    current_hour_utc = datetime.now(timezone.utc).hour
    if (20 <= current_hour_utc <= 22) or (os.environ.get("FORCE_HARVEST")=="true"):
        perform_grand_harvest(all_procs)
    
    print(f"\n[{datetime.now().strftime('%H:%M:%S')}] âœ… å®¡è®¡ä»»åŠ¡åœ†æ»¡å®Œæˆã€‚")
