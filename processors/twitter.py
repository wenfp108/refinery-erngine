import json
import math
import re
from datetime import datetime, timedelta

TABLE_NAME = "twitter_logs"
TARGET_TOTAL_QUOTA = 30 

# === ğŸ›‘ 1. æ”¿æ²»/åƒåœ¾å™ªéŸ³è¯ (æ ¸æ‰“å‡») ===
# åªè¦å‡ºç°ï¼Œåˆ†æ•°ç›´æ¥æ‰“ 1 æŠ˜
NOISE_KEYWORDS = [
    "woke", "maga", "democrat", "republican", "leftist", "right wing", "liberal", "conservative",
    "fascist", "communist", "socialist", "pronouns", "dei", "border crisis", "illegal",
    "trump", "biden", "harris", "vance", "pelosi", "schumer", "election", "ballot",
    "scandal", "epstein", "pedophile", "traitor", "shame", "disgrace", "culture war",
    "nazi", "hitler", "antisemitism", "zionist", "genocide"
]

# === ğŸ”° 2. å®è§‚è±å…è¯ (å…æ­»é‡‘ç‰Œ) ===
# æ”¿æ²»è´´é‡Œå¦‚æœæœ‰è¿™äº›è¯ï¼Œè¯´æ˜åœ¨èŠæ­£äº‹ï¼Œä¸é™æƒ
MACRO_IMMUNITY = [
    "fed", "federal reserve", "powell", "fomc", "rate", "interest", "cut", "hike",
    "tariff", "trade war", "sanction", "export", "import", "duty",
    "china", "taiwan", "russia", "ukraine", "israel", "iran", "war", "military",
    "stimulus", "debt", "deficit", "budget", "tax", "treasury", "bond", "yield",
    "bitcoin", "btc", "crypto", "ban", "regulation", "sec", "gensler", "etf",
    "executive order", "veto", "sign", "bill", "act", "law", "legislation",
    "nominate", "nominee", "appoint", "confirm", "supreme court"
]

# === ğŸ§  3. ç²¾å‡†è¯é¢˜è¯åº“ (æƒé‡ç«ä»·æ¨¡å¼) ===
# è¯è¶Šé•¿ã€è¶Šä¸“ä¸šï¼Œæƒé‡è¶Šé«˜ï¼Œé˜²æ­¢è¯¯åˆ¤
TOPIC_RULES = {
    "Crypto": [
        "bitcoin", "btc", "ethereum", "eth", "solana", "defi", "nft", "stablecoin", "usdc", "usdt",
        "etf flow", "blackrock", "layer2", "zk-rollup", "airdrop", "staking", "restaking", "memecoin",
        "binance", "coinbase", "satoshi", "vitalik", "on-chain analysis", "wallet", "altcoin"
    ],
    "AI/Tech": [
        "llm", "transformer", "genai", "generative ai", "inference", "training run", "pre-training",
        "gpt-5", "gpt-4", "claude", "gemini", "llama", "deepseek", "mistral", "anthropic", "openai",
        "nvidia", "nvda", "h100", "blackwell", "cuda", "gpu", "tpu", "asic", "compute",
        "tsmc", "asml", "semiconductor", "chip", "wafer", "Moore's law",
        "spacex", "starship", "falcon", "tesla", "tsla", "fsd", "optimus", "robot",
        "python", "rust", "github", "huggingface", "arxiv", "open source"
    ],
    "Science": [
        "nature journal", "science magazine", "arxiv", "peer review", "preprint",
        "nasa", "esa", "jwst", "supernova", "exoplanet", "quantum", "entanglement",
        "superconductor", "lk-99", "fusion energy", "iter", "plasma",
        "crispr", "mrna", "protein", "enzyme", "cancer research", "alzheimer", "longevity"
    ],
    "Macro": [
        "sp500", "nasdaq", "bond yield", "treasury", "curve inversion",
        "gold", "xau", "silver", "crude oil", "brent", "natural gas",
        "earnings call", "revenue", "guidance", "profit margin", "buyback", "dividend",
        "fomc", "fed funds", "powell", "cpi", "ppi", "pce", "inflation", "deflation", "stagflation",
        "gdp", "recession", "soft landing", "non-farm", "unemployment", "jobless", "payroll",
        "balance sheet", "quantitative tightening", "liquidity injection"
    ],
    "Geo": [
        "ukraine", "russia", "putin", "zelensky", "donbas", "kursk",
        "israel", "gaza", "hamas", "hezbollah", "iran", "tehran", "red sea", "houthi",
        "china", "xi jinping", "taiwan", "south china sea", "pla", "semiconductor sanction",
        "nato", "pentagon", "dod", "nuclear", "icbm", "drone warfare"
    ]
}

VIP_AUTHORS = [
    "Karpathy", "Yann LeCun", "Vitalik", "Paul Graham", "Naval", 
    "Eric Topol", "Huberman", "Lex Fridman", "Sam Altman", "Kobeissi Letter",
    "Michael Saylor", "Balaji"
]

def fmt_k(num):
    if not num: return "0"
    try: n = float(num)
    except: return "0"
    if n >= 1_000_000: return f"{n/1_000_000:.1f}M"
    if n >= 1_000: return f"{n/1_000:.1f}K"
    return str(int(n))

def to_iso_bj(date_str):
    try:
        utc_dt = datetime.strptime(date_str, '%a %b %d %H:%M:%S +0000 %Y')
        return (utc_dt + timedelta(hours=8)).isoformat()
    except: return datetime.now().isoformat()

def process(raw_data, path):
    items = raw_data if isinstance(raw_data, list) else [raw_data]
    refined_results = []
    for i in items:
        # åƒåœ¾è¿‡æ»¤ï¼šå¦‚æœæ­£æ–‡å¤ªçŸ­ä¸”æ²¡æœ‰é“¾æ¥ï¼Œç›´æ¥ä¸¢å¼ƒï¼ˆæ€æ‰ "Yes..." è¿™ç§æ°´è´´ï¼‰
        text = i.get('fullText', '')
        if len(text) < 10 and 'http' not in text:
            continue

        user = i.get('user', {})
        metrics = i.get('metrics', {})
        row = {
            "bj_time": to_iso_bj(i.get('createdAt')),
            "user_name": user.get('name'),
            "screen_name": user.get('screenName'),
            "followers_count": user.get('followersCount'),
            "full_text": text,
            "url": i.get('tweetUrl'), 
            "tags": i.get('tags', []),
            "likes": metrics.get('likes', 0),
            "retweets": metrics.get('retweets', 0),
            "bookmarks": metrics.get('bookmarks', 0),
            "raw_json": i 
        }
        refined_results.append(row)
    return refined_results

# ğŸ”¥ æ ¸å¿ƒï¼šä¸Šå¸æƒé‡ç®—æ³• 2.0 ğŸ”¥
def calculate_score_and_tag(item):
    text = (item.get('full_text') or "").lower()
    user = (item.get('user_name') or "")
    
    # 1. åŸºç¡€çƒ­åº¦ (ä¹¦ç­¾ x10, è½¬æ¨ x5, ç‚¹èµ x1)
    metrics = item.get('raw_json', {}).get('metrics', {})
    base_score = (metrics.get('retweets', 0) * 5) + \
                 (metrics.get('bookmarks', 0) * 10) + \
                 metrics.get('likes', 0)
    
    # 2. è¯é¢˜ç«ä»· (è§£å†³åˆ†ç±»å¹»è§‰)
    detected_topic = "General"
    max_keyword_len = 0 # åŒ¹é…åˆ°çš„å…³é”®è¯è¶Šé•¿ï¼Œç½®ä¿¡åº¦è¶Šé«˜
    
    for topic, keywords in TOPIC_RULES.items():
        for k in keywords:
            # å¿…é¡»æ˜¯ç‹¬ç«‹å•è¯åŒ¹é…ï¼Œé˜²æ­¢ "training" åŒ¹é…åˆ° "straining" (è™½ç„¶è‹±æ–‡è¾ƒå°‘è§ï¼Œä½†é€»è¾‘æ›´ä¸¥è°¨)
            if k in text:
                # ç®€å•çš„ä¼˜å…ˆçº§ï¼šå¦‚æœè¿™ä¸ªè¯æ¯”ä¹‹å‰åŒ¹é…åˆ°çš„è¯æ›´é•¿/æ›´å…·ä½“ï¼Œå°±é‡‡çº³è¿™ä¸ªåˆ†ç±»
                if len(k) > max_keyword_len:
                    detected_topic = topic
                    max_keyword_len = len(k)
    
    # 3. è¯­ä¹‰åŠ æƒ vs é™æƒ
    if detected_topic != "General":
        # å‘½ä¸­ç¡¬æ ¸æ¿å—ï¼šåŠ åˆ†
        base_score += 2000
        base_score *= 1.5
    else:
        # General æƒ©ç½šï¼šå¦‚æœæ˜¯æ°´è´´ï¼Œåˆ†æ•°æ‰“å¯¹æŠ˜
        # é™¤éå®ƒæ˜¯è¶…çº§å¤§çƒ­ç‚¹ï¼Œå¦åˆ™åˆ«æƒ³æŒ¤æ‰ç¡¬æ ¸å†…å®¹
        base_score *= 0.5 

    # 4. æ”¿æ²»æ’æ¯’
    has_noise = False
    for noise in NOISE_KEYWORDS:
        if noise in text:
            has_noise = True
            break
            
    if has_noise:
        is_immune = False
        for safe in MACRO_IMMUNITY:
            if safe in text:
                is_immune = True
                break
        if not is_immune:
            base_score *= 0.1 # æ ¸æ‰“å‡»
            detected_topic = "Politics" # å¼ºåˆ¶æ ‡è®°
            
    # 5. VIP åŠ æˆ
    for vip in VIP_AUTHORS:
        if vip.lower() in user.lower():
            base_score += 5000
            break
            
    return base_score, detected_topic

def get_hot_items(supabase, table_name):
    yesterday = (datetime.now() - timedelta(hours=24)).isoformat()
    try:
        res = supabase.table(table_name).select("*").gt("bj_time", yesterday).execute()
        all_tweets = res.data if res.data else []
    except Exception as e: return {}

    if not all_tweets: return {}

    unique_map = {}
    for t in all_tweets:
        key = t.get('url') or (t.get('user_name'), t.get('full_text'))
        if key not in unique_map:
            unique_map[key] = t
    tweets = list(unique_map.values())

    scored_tweets = []
    for t in tweets:
        score, topic = calculate_score_and_tag(t)
        t['_score'] = score
        t['_topic'] = topic
        scored_tweets.append(t)
        
    scored_tweets.sort(key=lambda x: x['_score'], reverse=True)
    
    # ğŸ›¡ï¸ å¤šæ ·æ€§ç†”æ–­ (æ¯äººæœ€å¤š 3 æ¡)
    final_list = []
    author_counts = {}
    
    for t in scored_tweets:
        if len(final_list) >= TARGET_TOTAL_QUOTA:
            break
            
        author = t['user_name']
        if author_counts.get(author, 0) >= 3:
            continue
            
        final_list.append(t)
        author_counts[author] = author_counts.get(author, 0) + 1
        
    # ç”Ÿæˆå¤§è¡¨
    header = "| ä¿¡å· | ğŸ·ï¸ æ ‡ç­¾ | çƒ­åº¦ | åšä¸» | æ‘˜è¦ | ğŸ”— |\n| :--- | :--- | :--- | :--- | :--- | :--- |"
    rows = []
    
    for t in final_list:
        score_display = fmt_k(t['_score'])
        
        # æ ‡ç­¾ç¾åŒ–
        topic_raw = t['_topic']
        if topic_raw == "General": topic_str = "General" # ä¸åŠ ç²—
        else: topic_str = f"**{topic_raw}**" # ç¡¬æ ¸æ ‡ç­¾åŠ ç²—
        
        heat = f"â¤ï¸ {fmt_k(t.get('likes',0))}<br>ğŸ” {fmt_k(t.get('retweets',0))}" 
        user = t['user_name']
        text = t['full_text'].replace('\n', ' ')[:70] + "..."
        url = t['url']
        
        rows.append(f"| **{score_display}** | {topic_str} | {heat} | {user} | {text} | [ğŸ”—]({url}) |")

    return {"ğŸ† å…¨åŸŸç²¾é€‰ (Top 30)": {"header": header, "rows": rows}}
