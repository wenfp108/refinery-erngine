import json
from datetime import datetime, timedelta

# å¯¹åº” Supabase é‡Œçš„è¡¨å
TABLE_NAME = "papers_logs"

def fmt_k(num):
    if not num: return "0"
    try: n = float(num)
    except: return "0"
    if n >= 1_000: return f"{n/1_000:.1f}K"
    return str(int(n))

# === 1. æ•°æ®æ¸…æ´—é€»è¾‘ ===
def process(raw_data, path):
    # å…¼å®¹å¤„ç†ï¼šæœ‰äº› JSON æ˜¯ dict (å« meta)ï¼Œæœ‰äº›å¯èƒ½æ˜¯ list
    data = raw_data if isinstance(raw_data, dict) else {}
    items = data.get("items", [])
    meta = data.get("meta", {})
    
    # è·å–æ‰«ææ—¶é—´ï¼Œå¦‚æœæ²¡æœ‰åˆ™ç”¨å½“å‰æ—¶é—´
    scanned_at = meta.get("scanned_at_bj")
    if not scanned_at:
        scanned_at = datetime.now().isoformat()
        
    refined_results = []
    for i in items:
        # æå– metricsï¼Œé˜²æ­¢ key ä¸å­˜åœ¨æŠ¥é”™
        metrics = i.get("metrics", {})
        
        row = {
            "bj_time": scanned_at,
            "title": i.get("title"),
            "journal": i.get("journal"),
            # åŒºåˆ† â˜¢ï¸ NUCLEAR å’Œ âš¡ EARLY_SIGNAL
            "signal_type": i.get("type", "General"), 
            "citations": int(metrics.get("citations", 0)),
            "impact_factor": float(metrics.get("impact_factor", 0.0)),
            # æ•°ç»„è½¬ JSON å­—ç¬¦ä¸²
            "strategies": i.get("strategies", []), 
            "url": i.get("url"),
            "reason": i.get("reason"),
            "raw_json": i
        }
        refined_results.append(row)
    return refined_results

# === 2. æˆ˜æŠ¥ç”Ÿæˆé€»è¾‘ ===
def get_hot_items(supabase, table_name):
    # åªçœ‹æœ€è¿‘ 24 å°æ—¶çš„æ•°æ®
    yesterday = (datetime.now() - timedelta(hours=24)).isoformat()
    try:
        res = supabase.table(table_name).select("*").gt("bj_time", yesterday).execute()
        all_papers = res.data if res.data else []
    except Exception as e:
        print(f"Papers DB Error: {e}")
        return {}
    
    if not all_papers: return {}

    # å»é‡é€»è¾‘
    unique_map = {}
    for p in all_papers:
        title = p.get("title")
        if not title: continue
        unique_map[title] = p
    
    papers = list(unique_map.values())
    
    # æ’åºé€»è¾‘ï¼šæ ¸çˆ†çº§ç½®é¡¶ï¼Œç„¶åæŒ‰å¼•ç”¨æ•°é™åº
    def sort_key(p):
        score = p.get("citations", 0)
        if "NUCLEAR" in p.get("signal_type", ""):
            score += 100000 
        return score

    papers.sort(key=sort_key, reverse=True)
    
    # æ„å»º Markdown è¡¨æ ¼
    # ğŸ”¥ ä¿®æ”¹ï¼šç§»é™¤â€œæœŸåˆŠâ€åˆ—ï¼Œç²¾ç®€ä¸ºæ ¸å¿ƒä¿¡æ¯
    header = "| ä¿¡å· | æ ‡é¢˜ | å¼•ç”¨ | æ ‡ç­¾ (å…³é”®è¯) | ğŸ”— |\n| :--- | :--- | :--- | :--- | :--- |"
    rows = []
    
    for p in papers:
        s_type = p.get("signal_type","")
        # å›¾æ ‡ç¾åŒ–
        if "NUCLEAR" in s_type:
            icon = "â˜¢ï¸ **NUCLEAR**"
        elif "EARLY" in s_type:
            icon = "âš¡ Early"
        else:
            icon = "ğŸ“„ Paper"
            
        title = p.get("title", "")
        if len(title) > 65: title = title[:65] + "..."
            
        cite = fmt_k(p.get("citations", 0))
        
        # å¤„ç†æ ‡ç­¾æ˜¾ç¤º
        tags = p.get("strategies", [])
        if isinstance(tags, str):
            try: tags = json.loads(tags)
            except: tags = []
        
        # æ ‡ç­¾åŠ ç²—æ˜¾ç¤ºï¼Œè§†è§‰æ›´æ¸…æ™°
        tag_str = ", ".join([f"**{t}**" for t in tags[:2]])
        
        url = p.get("url", "#")
        
        # ğŸ”¥ ä¿®æ”¹ï¼šä¸åŒ…å« journal å­—æ®µ
        rows.append(f"| {icon} | {title} | {cite} | {tag_str} | [ğŸ”—]({url}) |")
        
    return {"ğŸ”¬ Science Radar (ç§‘ç ”å‰å“¨)": {"header": header, "rows": rows}}
