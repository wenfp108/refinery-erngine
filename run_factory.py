import os
import json
import pandas as pd
from datetime import datetime, timedelta, timezone
from supabase import create_client
from factory import UniversalFactory  # å¯¼å…¥é€šç”¨å·¥å‚ç±»

# === âš™ï¸ é…ç½®åŒº ===
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")

# ä½ çš„ä¸­å¤®é“¶è¡Œåœ¨ GitHub Action é‡Œçš„ç›¸å¯¹è·¯å¾„
VAULT_PATH = "../vault"

# ä½ æ‰€æœ‰çš„æƒ…æŠ¥æºè¡¨å
TARGET_TABLES = [
    "polymarket_logs",
    "twitter_logs",
    "reddit_logs",
    "github_logs",
    "papers_logs"
]

def fetch_fresh_data(table_name, minutes=70):
    """
    ä»æŒ‡å®šè¡¨æå–æœ€è¿‘ N åˆ†é’Ÿçš„æ•°æ®
    """
    try:
        supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
        
        # âœ… ä¿®å¤ï¼šå¼ºåˆ¶å¯¹é½åŒ—äº¬æ—¶é—´ (UTC+8)
        # ç¡®ä¿ä¸ refinery.py å†™å…¥çš„ bj_time æ ¼å¼ä¸€è‡´ï¼Œé¿å…å­—ç¬¦ä¸²æ¯”è¾ƒæ—¶å‡ºç° 8 å°æ—¶åå·®
        bj_now = datetime.now(timezone(timedelta(hours=8)))
        cutoff_time = (bj_now - timedelta(minutes=minutes)).isoformat()
        
        print(f"ğŸ£ [{table_name}] æ­£åœ¨æ‰«ææ–°æ•°æ® (é˜ˆå€¼: {cutoff_time})...")
        
        # é™åˆ¶å•æ¬¡æœ€å¤§è·å– 1000 æ¡
        res = supabase.table(table_name)\
            .select("*")\
            .gt("bj_time", cutoff_time)\
            .limit(1000)\
            .execute()
            
        data = res.data
        if data:
            print(f"   âœ… æ•è· {len(data)} æ¡ä¿¡å·")
            return data
        else:
            print(f"   ğŸ’¤ æ— æ–°å¢ä¿¡å·")
            return []
            
    except Exception as e:
        print(f"   âš ï¸ [{table_name}] è¯»å–å¤±è´¥: {e}")
        return []

def main():
    bj_now_str = datetime.now(timezone(timedelta(hours=8))).isoformat()
    print(f"ğŸš€ [Cognitive Factory] å¯åŠ¨æ—¶é—´: {bj_now_str}")
    
    all_signals = []
    
    # 1. éå†æ‰€æœ‰æºï¼Œæ”¶é›†æ–°é²œåŸæ–™
    for table in TARGET_TABLES:
        rows = fetch_fresh_data(table)
        if rows:
            all_signals.extend(rows)
            
    if not all_signals:
        print("ğŸ“­ æœ¬è½®å·¡æ£€æœªå‘ç°ä»»ä½•æ–°æ•°æ®ï¼Œå·¥å‚ä¼‘çœ ã€‚")
        return

    print(f"ğŸ“¦ åŸæ–™å‡†å¤‡å®Œæ¯•ï¼Œå…±è®¡ {len(all_signals)} æ¡æ··åˆä¿¡å·ã€‚")

    # 2. è½¬æ¢ä¸º DataFrame å¹¶è¿›è¡Œé¢„å¤„ç†
    df = pd.DataFrame(all_signals)
    temp_file = "temp_run_batch.parquet"
    
    # âœ… æ ¸å¿ƒä¿®å¤ï¼šå¼ºåˆ¶å°† raw_json åˆ—è½¬æ¢ä¸ºçº¯å­—ç¬¦ä¸²æ ¼å¼
    # è§£å†³ pyarrow æ— æ³•æ··åˆå¤„ç† dict å’Œ string å¯¼è‡´çš„ ArrowInvalid æŠ¥é”™
    if 'raw_json' in df.columns:
        df['raw_json'] = df['raw_json'].apply(
            lambda x: json.dumps(x, ensure_ascii=False) if isinstance(x, (dict, list)) else str(x)
        )
    
    # å…¼å®¹æ€§ï¼šç¡®ä¿æ•°å€¼å­—æ®µç±»å‹ç»Ÿä¸€ï¼Œé˜²æ­¢ç©ºå€¼æŠ¥é”™
    numeric_cols = ['volume', 'liquidity', 'vol24h', 'day_change', 'stars', 'citations', 'score']
    for col in numeric_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
            
    # ä¿å­˜ä¸ºä¸´æ—¶ Parquet
    try:
        df.to_parquet(temp_file, engine='pyarrow', index=False)
    except Exception as e:
        print(f"âŒ Parquet å†™å…¥å¤±è´¥ (æ•°æ®ç»“æ„å¼‚å¸¸): {e}")
        return

    # 3. å”¤é†’å¤§å¸ˆï¼Œå¼€å·¥
    try:
        # masters_path="masters" å¯¹åº” workflow é‡Œå¤åˆ¶è¿‡æ¥çš„æ’ä»¶ç›®å½•
        factory = UniversalFactory(masters_path="masters")
        
        print("ğŸ­ æµæ°´çº¿å…¨é€Ÿè¿è½¬ä¸­...")
        factory.process_and_ship(
            input_raw=temp_file, 
            vault_path=VAULT_PATH
        )
        
    except Exception as e:
        print(f"âŒ å·¥å‚è¿è¡Œä¸¥é‡é”™è¯¯: {e}")
        
    finally:
        # 4. æ¸…ç†ç°åœº
        if os.path.exists(temp_file):
            os.remove(temp_file)
            print("ğŸ§¹ ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†ã€‚")

if __name__ == "__main__":
    if not SUPABASE_URL or not SUPABASE_KEY:
        print("âŒ [é”™è¯¯] ç¯å¢ƒå˜é‡ç¼ºå¤± (SUPABASE_URL/KEY)")
    else:
        main()
