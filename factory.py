import os, json, base64, requests, importlib.util, sys
import pandas as pd
import io
from datetime import datetime, timedelta, timezone
from supabase import create_client
from github import Github, Auth

# === ğŸ›¡ï¸ 1. æ ¸å¿ƒé…ç½® ===
PRIVATE_BANK_ID = "wenfp108/Central-Bank" 
GITHUB_TOKEN = os.environ.get("GH_PAT") 
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")

if not all([GITHUB_TOKEN, SUPABASE_URL, SUPABASE_KEY]):
    sys.exit("âŒ [å®¡è®¡å¼‚å¸¸] ç¯å¢ƒå˜é‡ç¼ºå¤±ã€‚")

supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
auth = Auth.Token(GITHUB_TOKEN)
gh_client = Github(auth=auth)
private_repo = gh_client.get_repo(PRIVATE_BANK_ID)

# === ğŸ§© 2. æ’ä»¶å‘ç°ç³»ç»Ÿ (å·²ä¿®æ”¹ï¼šå¼ºåˆ¶æŒ‡å‘ raw_signals) ===
def get_all_processors():
    procs = {}
    proc_dir = "./processors"
    if not os.path.exists(proc_dir): return procs
    for filename in os.listdir(proc_dir):
        if filename.endswith(".py") and not filename.startswith("__"):
            name = filename[:-3]
            try:
                spec = importlib.util.spec_from_file_location(f"mod_{name}", os.path.join(proc_dir, filename))
                mod = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(mod)
                procs[name] = {
                    "module": mod,
                    "source_name": name,  # è®°å½•æ¥æºå (twitter, github...)
                    "table_name": "raw_signals",  # ğŸ”¥ å¼ºåˆ¶ç»Ÿä¸€è¡¨å
                }
            except Exception as e: print(f"âš ï¸ æ’ä»¶ {name} åŠ è½½å¤±è´¥: {e}")
    return procs

# === â±ï¸ è¾…åŠ©ï¼šæ£€æŸ¥æ•°æ®æ–°é²œåº¦ ===
def get_data_freshness(table_name, source_name=None):
    try:
        query = supabase.table(table_name).select("bj_time").neq("bj_time", "null")
        
        # å¦‚æœæ˜¯ raw_signalsï¼Œéœ€è¦æŒ‰ signal_type è¿‡æ»¤
        if table_name == "raw_signals" and source_name:
            query = query.eq("signal_type", source_name)
            
        res = query.order("bj_time", desc=True).limit(1).execute()
        
        if not res.data: return (False, 9999, "æ— æ•°æ®")
        
        last_time_str = res.data[0]['bj_time']
        if not last_time_str: return (False, 9999, "æ— æ—¶é—´æˆ³")

        try:
            last_time_str = last_time_str.replace('Z', '+00:00')
            last_time = datetime.fromisoformat(last_time_str)
        except:
            return (False, 9999, last_time_str)
        
        now = datetime.now(timezone(timedelta(hours=8)))
        if last_time.tzinfo is None:
            last_time = last_time.replace(tzinfo=timezone(timedelta(hours=8)))
        
        diff = now - last_time
        minutes_ago = int(diff.total_seconds() / 60)
        
        return (minutes_ago <= 65, minutes_ago, last_time.strftime('%H:%M'))
    except Exception as e:
        return (True, 0, "CheckError")

# === ğŸ”¥ 3. æˆ˜æŠ¥å·¥å‚ ===
def generate_hot_reports(processors_config):
    bj_now = datetime.now(timezone(timedelta(hours=8)))
    year = bj_now.strftime('%Y')
    month = bj_now.strftime('%m')
    day = bj_now.strftime('%d')
    hour = bj_now.strftime('%H')
    
    file_name = f"{hour}ç‚¹æˆ˜æŠ¥.md"
    report_path = f"reports/{year}/{month}/{day}/{file_name}"
    
    date_display = bj_now.strftime('%Y-%m-%d %H:%M')
    md_report = f"# ğŸš€ Architect's Alpha æƒ…æŠ¥å®¡è®¡ ({date_display})\n\n"
    md_report += "> **æœºåˆ¶è¯´æ˜**ï¼šå…¨æºæ™ºèƒ½å»é‡ | èµ„é‡‘æµå‘ä¼˜å…ˆ | è‡ªåŠ¨å½’æ¡£\n\n"

    has_content = False
    active_sources_count = 0

    for source_name, config in processors_config.items():
        if hasattr(config["module"], "get_hot_items"):
            try:
                table = config["table_name"]
                # ä¼ å…¥ source_name è¿›è¡Œè¿‡æ»¤
                is_fresh, mins_ago, last_update_time = get_data_freshness(table, source_name)
                
                if not is_fresh and mins_ago > 720: 
                    continue 

                # æ³¨æ„ï¼šget_hot_items é‡Œé¢çš„é€»è¾‘å¯èƒ½è¿˜æ²¡é€‚é… raw_signalsï¼Œ
                # ä½† Factory.py æ˜¯ä¸»æˆ˜åœºï¼Œè¿™ä¸ªæˆ˜æŠ¥åŠŸèƒ½å¯ä»¥æš‚æ—¶ä½œä¸ºè¾…åŠ©ã€‚
                sector_data = config["module"].get_hot_items(supabase, table)
                if not sector_data: continue

                has_content = True
                active_sources_count += 1
                
                freshness_tag = "" if is_fresh else f" (âš ï¸ æ•°æ®æ»å {int(mins_ago/60)}h)"
                md_report += f"## ğŸ“¡ æ¥æºï¼š{source_name.upper()}{freshness_tag}\n"
                
                for sector, data in sector_data.items():
                    md_report += f"### ğŸ·ï¸ æ¿å—ï¼š{sector}\n"
                    if isinstance(data, dict):
                        if "header" in data: md_report += data["header"] + "\n"
                        if "rows" in data and isinstance(data["rows"], list):
                            for row in data["rows"]: md_report += row + "\n"
                    elif isinstance(data, list):
                        md_report += "| ä¿¡å· | å†…å®¹ | ğŸ”— |\n| :--- | :--- | :--- |\n"
                        for item in data:
                            md_report += f"| {item.get('score','-')} | {item.get('full_text','-')} | [ğŸ”—]({item.get('url','#')}) |\n"
                    md_report += "\n"
            except Exception as e:
                pass 

    if not has_content:
        md_report += "\n\n**ğŸ›‘ æœ¬è½®æ‰«æå…¨åŸŸé™é»˜ï¼Œè¯·æŸ¥é˜…å†å²å½’æ¡£ã€‚**"

    try:
        try:
            old = private_repo.get_contents(report_path)
            private_repo.update_file(old.path, f"ğŸ“Š Update: {file_name}", md_report, old.sha)
            print(f"ğŸ“ æˆ˜æŠ¥æ›´æ–°ï¼š{report_path}")
        except:
            private_repo.create_file(report_path, f"ğŸš€ New: {file_name}", md_report)
            print(f"ğŸ“ æˆ˜æŠ¥åˆ›å»ºï¼š{report_path}")
    except Exception as e: 
        print(f"âŒ å†™å…¥å¤±è´¥: {e}")

# === ğŸšœ 4. æ»šåŠ¨æ”¶å‰² (é€‚é… raw_signals) ===
def perform_grand_harvest(processors_config):
    print("â° è§¦å‘æ¯æ—¥æ»šåŠ¨æ”¶å‰² (Archive & Purge)...")
    cutoff_date = (datetime.now() - timedelta(days=7))
    cutoff_str = cutoff_date.isoformat()
    date_tag = cutoff_date.strftime('%Y%m%d')

    # åªéœ€è¦å¯¹ raw_signals åšä¸€æ¬¡æ¸…ç†å³å¯
    table = "raw_signals"
    try:
        res = supabase.table(table).select("*").lt("created_at", cutoff_str).execute()
        data = res.data
        
        if data:
            df = pd.DataFrame(data)
            buffer = io.BytesIO()
            df.to_parquet(buffer, index=False, engine='pyarrow', compression='snappy')
            
            year_month = cutoff_date.strftime('%Y/%m')
            archive_path = f"archive/{year_month}/{table}_{date_tag}.parquet"
            
            private_repo.create_file(
                path=archive_path,
                message=f"ğŸ›ï¸ Archive: {table} batch",
                content=buffer.getvalue(),
                branch="main" 
            )
            
            ids = [item['id'] for item in data if 'id' in item]
            if ids:
                for i in range(0, len(ids), 500):
                    supabase.table(table).delete().in_("id", ids[i:i+500]).execute()
                print(f"   ğŸ—‘ï¸ {table}: å·²æ¸…ç† {len(ids)} æ¡è¿‡æœŸæ•°æ®")
    except Exception as e:
        pass

# === ğŸ¦ 5. æ¬è¿é€»è¾‘ (æ ¸å¿ƒä¿®æ”¹ï¼šæ³¨å…¥ signal_type) ===
def process_and_upload(path, sha, config):
    check = supabase.table("processed_files").select("file_sha").eq("file_sha", sha).execute()
    if check.data: return 0
    try:
        content_file = private_repo.get_contents(path)
        raw_data = json.loads(base64.b64decode(content_file.content).decode('utf-8'))
        
        # è°ƒç”¨ Processor æ¸…æ´—æ•°æ®
        items = config["module"].process(raw_data, path)
        count = len(items) if items else 0
        
        if items:
            # ğŸ”¥ æ³¨å…¥æ ¸å¿ƒå­—æ®µ signal_type
            for item in items:
                item['signal_type'] = config["source_name"]
                
                # å…¼å®¹æ€§å¤„ç†ï¼šç¡®ä¿ raw_json å­˜åœ¨ (å¦‚æœ processor æ²¡ç”Ÿæˆ)
                if 'raw_json' not in item:
                    item['raw_json'] = item.copy() # ç®€å•å¤‡ä»½

            # åˆ†æ‰¹å†™å…¥ raw_signals
            for i in range(0, len(items), 500):
                supabase.table("raw_signals").insert(items[i : i+500]).execute()
            
            # è®°å½•æ–‡ä»¶å·²å¤„ç†
            supabase.table("processed_files").upsert({
                "file_sha": sha, 
                "file_path": path,
                "engine": config["source_name"],
                "item_count": count
            }).execute()
            return count
    except Exception as e: 
        print(f"âŒ å¤„ç†æ–‡ä»¶ {path} å¤±è´¥: {e}")
    return 0

def sync_bank_to_sql(processors_config, full_scan=False):
    current_time = datetime.now().strftime('%H:%M:%S')
    mode_str = "å…¨é‡è¡¥å½•" if full_scan else "1å°æ—¶å¢é‡"
    print(f"[{current_time}] ğŸ¦ å·¡æ£€å¼€å§‹: {mode_str}æå–")
    stats = {name: 0 for name in processors_config.keys()}
    
    if full_scan:
        try:
            contents = private_repo.get_contents("")
            while contents:
                file_content = contents.pop(0)
                if file_content.type == "dir":
                    contents.extend(private_repo.get_contents(file_content.path))
                elif file_content.name.endswith(".json"):
                    source_key = file_content.path.split('/')[0]
                    if source_key in processors_config:
                        added = process_and_upload(file_content.path, file_content.sha, processors_config[source_key])
                        stats[source_key] += added
        except Exception as e: print(f"âŒ Scan Error: {e}")
    else:
        since = datetime.now(timezone.utc) - timedelta(hours=24) # ç¨å¾®å¤šçœ‹ä¸€ç‚¹æ—¶é—´ï¼Œé˜²æ­¢æ¼
        commits = private_repo.get_commits(since=since)
        for commit in commits:
            for f in commit.files:
                if f.filename.endswith('.json'):
                    source_key = f.filename.split('/')[0]
                    if source_key in processors_config:
                        added = process_and_upload(f.filename, f.sha, processors_config[source_key])
                        stats[source_key] += added

    for source, count in stats.items():
        if count > 0: print(f"âœ… {source} (+{count}) -> raw_signals")

if __name__ == "__main__":
    all_procs = get_all_processors()
    is_full_scan = (os.environ.get("FORCE_FULL_SCAN") == "true")
    
    sync_bank_to_sql(all_procs, full_scan=is_full_scan)
    generate_hot_reports(all_procs)
    perform_grand_harvest(all_procs)
    
    print(f"[{datetime.now().strftime('%H:%M:%S')}] âœ… å®¡è®¡ä»»åŠ¡åœ†æ»¡å®Œæˆã€‚")
