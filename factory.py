import pandas as pd
import hashlib, json, os, requests, subprocess, time, sys
from pathlib import Path
from datetime import datetime, timezone, timedelta
from concurrent.futures import ThreadPoolExecutor
from supabase import create_client
import importlib.util

class UniversalFactory:
    def __init__(self, masters_path="masters"):
        self.masters_path = Path(masters_path)
        self.masters = self._load_masters()
        # é…ç½®
        self.api_key = os.environ.get("SILICON_FLOW_KEY") 
        self.api_url = "https://api.siliconflow.cn/v1/chat/completions"
        self.supabase_url = os.environ.get("SUPABASE_URL")
        self.supabase_key = os.environ.get("SUPABASE_KEY")
        self.v3_model = "deepseek-ai/DeepSeek-V3"
        self.vault_path = None
        self.memory = {} # ğŸ§  è®¤çŸ¥è®°å¿†åº“

    def _load_masters(self):
        masters = {}
        if not self.masters_path.exists(): 
            self.masters_path.mkdir(exist_ok=True)
            return masters
        for file_path in self.masters_path.glob("*.py"):
            if file_path.name.startswith("__"): continue
            try:
                name = file_path.stem
                spec = importlib.util.spec_from_file_location(name, file_path)
                module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(module)
                if hasattr(module, 'audit'): masters[name] = module
                print(f"âœ… å·²åŠ è½½ Master: {name}")
            except Exception as e:
                print(f"âš ï¸ Master {file_path.name} åŠ è½½å¤±è´¥: {e}")
        return masters

    def build_memory(self, output_file):
        """ğŸ§  æ‰«æä»Šæ—¥å·²äº§å‡ºæ•°æ®ï¼Œæ„å»ºçŸ­æœŸè®°å¿†ç´¢å¼•"""
        if not output_file.exists(): return
        print(f"ğŸ§ æ­£åœ¨åŒæ­¥ä»Šæ—¥è®°å¿†: {output_file.name}...")
        try:
            with open(output_file, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        data = json.loads(line)
                        tid = data.get('topic_id')
                        m = data.get('master')
                        if tid and m:
                            if tid not in self.memory: self.memory[tid] = {}
                            self.memory[tid][m] = data.get('output', "")
                    except: continue
            print(f"âœ… è®°å¿†æ„å»ºå®Œæˆï¼Œæ¶‰åŠ {len(self.memory)} ä¸ªä¸»é¢˜")
        except Exception as e:
            print(f"âš ï¸ è®°å¿†æ„å»ºä¸­æ–­: {e}")

    def git_push_assets(self):
        if not self.vault_path: return
        cwd = self.vault_path
        print("ğŸ”„ [Git] æ­£åœ¨æ‰§è¡Œè¿½åŠ åŒæ­¥ (Rebase Mode)...")
        subprocess.run(["git", "pull", "origin", "main", "--rebase"], cwd=cwd, check=False)
        subprocess.run(["git", "add", "."], cwd=cwd, check=False)
        if subprocess.run(["git", "diff", "--cached", "--quiet"], cwd=cwd).returncode != 0:
            msg = f"ğŸ§  Cognitive Audit: {datetime.now().strftime('%H:%M:%S')}"
            subprocess.run(["git", "commit", "-m", msg], cwd=cwd, check=False)
            subprocess.run(["git", "pull", "origin", "main", "--rebase"], cwd=cwd, check=False)
            res = subprocess.run(["git", "push", "origin", "main"], cwd=cwd, check=False)
            if res.returncode == 0: print("âœ… [Git] è®¤çŸ¥èµ„äº§å·²å®‰å…¨è¿½åŠ ")
        else:
            print("ğŸ’¤ [Git] æ— æ–°å˜åŒ–")

    def call_ai(self, model, sys_prompt, usr_prompt):
        if not self.api_key: return "ERROR", "No Key"
        headers = {"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}
        payload = {
            "model": model, 
            "messages": [{"role": "system", "content": sys_prompt}, {"role": "user", "content": usr_prompt}],
            "temperature": 0.7, "max_tokens": 1500
        }
        try:
            res = requests.post(self.api_url, json=payload, headers=headers, timeout=60).json()
            return "SUCCESS", res['choices'][0]['message']['content']
        except Exception as e: return "ERROR", str(e)

    def audit_process(self, row, processed_ids):
        # 1. è¯†åˆ«ä¸»é¢˜å”¯ä¸€ IDï¼ˆè·¨æ—¶é—´çº¿è¿½è¸ªçš„æ ¸å¿ƒï¼‰
        topic_id = row.get('url') or row.get('slug') or row.get('repo_name') or "unknown"
        source = row.get('signal_type', 'unknown').lower()
        
        # 2. æ„å»ºè¾“å…¥
        parts = [f"ã€Source: {source.upper()}ã€‘"]
        if source == 'github':
            parts.append(f"é¡¹ç›®: {row.get('repo_name')} | Stars: {row.get('stars')} | æè¿°: {row.get('full_text')}")
        elif source == 'polymarket':
            raw = row.get('raw_json')
            if isinstance(raw, str): 
                try: raw = json.loads(raw)
                except: raw = {}
            parts.append(f"é¢„æµ‹: {row.get('title')} | ä»·æ ¼: {row.get('prices') or raw.get('outcome_prices')} | æµåŠ¨æ€§: ${raw.get('liquidity')}")
        else: # Twitter/Reddit
            parts.append(f"ç”¨æˆ·: {row.get('user_name') or row.get('subreddit')} | å†…å®¹: {row.get('full_text') or row.get('title')}")
        
        input_content = "\n".join(parts)
        ref_id = hashlib.sha256(input_content.encode()).hexdigest()
        
        # å¦‚æœå“ˆå¸Œå®Œå…¨ä¸€æ ·ï¼Œè¯´æ˜æ•°æ®æ²¡å˜ï¼Œè·³è¿‡ä»¥çœé’±
        if ref_id in processed_ids: return []

        results = []
        def ask_v3(s, u):
            st, r = self.call_ai(self.v3_model, s, u)
            if st == "SUCCESS" and "### Output" in r:
                return r.split("### Output")[0].replace("### Thought","").strip(), r.split("### Output")[1].strip()
            return "Analysis", r

        # 3. å¤§å¸ˆä¼šå®¡ + æ¼‚ç§»æ£€æµ‹
        for name, mod in self.masters.items():
            # ğŸ” æ£€ç´¢å†å²è®°å¿†
            prev_opinion = self.memory.get(topic_id, {}).get(name)
            drift_context = ""
            if prev_opinion:
                drift_context = f"\n\n[å†å²è®°å¿†]ï¼šä½ æ­¤å‰å¯¹è¯¥ä¸»é¢˜çš„è§‚ç‚¹æ˜¯ï¼š'{prev_opinion}'ã€‚è‹¥å½“å‰æ•°æ®è§¦å‘äº†ä½ çš„è§‚ç‚¹è½¬å‘ï¼Œè¯·åœ¨ Thought ä¸­è¯¦è¿°é€»è¾‘å˜åŒ–ï¼Œå¹¶åœ¨ Output å¼€å¤´æ ‡è®° [DRIFT_DETECTED]ã€‚"

            try:
                if hasattr(mod, 'audit'):
                    # æ³¨å…¥å†å²ä¸Šä¸‹æ–‡
                    row['_drift_context'] = drift_context
                    t, o = mod.audit(row, ask_v3)
                    if t and o:
                        results.append(json.dumps({
                            "topic_id": topic_id,
                            "ref_id": ref_id,
                            "source": source,
                            "master": name,
                            "drift": "[DRIFT_DETECTED]" in o,
                            "thought": t,
                            "output": o,
                            "prev_opinion": prev_opinion
                        }, ensure_ascii=False))
                        print(f"ğŸ’¡ [{name}] {'ğŸ”„ æ¼‚ç§»æ£€æµ‹' if '[DRIFT_DETECTED]' in o else 'æ´å¯Ÿç”Ÿæˆ'}: {topic_id[:30]}...")
            except: continue
        return results

    def process_and_ship(self, vault_path="vault"):
        self.vault_path = Path(vault_path)
        (self.vault_path / "instructions").mkdir(parents=True, exist_ok=True)
        
        day_str = datetime.now().strftime('%Y%m%d')
        output_file = self.vault_path / "instructions" / f"teachings_{day_str}.jsonl"
        
        # 1. åˆå§‹åŒ– Git å’Œ è®°å¿†
        self.configure_git()
        self.build_memory(output_file)
        
        processed_ids = set()
        if output_file.exists():
            with open(output_file, 'r', encoding='utf-8') as f:
                for line in f:
                    try: processed_ids.add(json.loads(line).get('ref_id'))
                    except: pass

        # 2. æŠ“å–æ–°é²œç²¾é”ä¿¡å· (æ­¤å¤„ç•¥å» fetch_elite_signals é€»è¾‘ï¼Œä¿æŒæ‚¨åŸæœ‰çš„å³å¯)
        from refinery import create_client as supabase_client
        supabase = supabase_client(self.supabase_url, self.supabase_key)
        # è¿™é‡Œæ¨¡æ‹ŸæŠ“å–é€»è¾‘ï¼Œå»ºè®®ä¿ç•™æ‚¨åŸæœ‰çš„ fetch_elite_signals å‡½æ•°
        signals = self.fetch_elite_signals() 

        if not signals: return

        print(f"ğŸš€ å¯åŠ¨è®¤çŸ¥å®¡è®¡æµæ°´çº¿: {len(signals)} ä¸ªä¿¡å·å¾…å¤„ç†...")

        # 3. 20 çº¿ç¨‹å¹¶å‘åŠ å·¥
        batch_size = 50
        for i in range(0, len(signals), batch_size):
            chunk = signals[i : i + batch_size]
            with ThreadPoolExecutor(max_workers=20) as executor:
                res = list(executor.map(lambda r: self.audit_process(r, processed_ids), chunk))
            
            added = []
            for r_list in res:
                if r_list: added.extend(r_list)
            
            if added:
                with open(output_file, 'a', encoding='utf-8') as f:
                    f.write('\n'.join(added) + '\n')
                self.git_push_assets() # å®æ—¶è¿½åŠ 

    def configure_git(self):
        subprocess.run(["git", "config", "--global", "user.email", "bot@factory.com"], check=False)
        subprocess.run(["git", "config", "--global", "user.name", "Cognitive Bot"], check=False)

    def fetch_elite_signals(self):
        # è¯·ä¿ç•™æ‚¨ä¹‹å‰ factory.py ä¸­å®Œæ•´çš„ fetch_elite_signals é€»è¾‘
        # æ­¤å¤„ä¸ºç¤ºæ„ï¼Œå®é™…è¿è¡Œæ—¶è¯·å°†åŸæœ‰çš„ fetch ä»£ç ç²˜è´´äºæ­¤
        pass

if __name__ == "__main__":
    factory = UniversalFactory()
    factory.process_and_ship()
