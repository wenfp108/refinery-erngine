import pandas as pd
import hashlib, json, os, requests, subprocess, time
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from supabase import create_client

class UniversalFactory:
    def __init__(self, masters_path="masters"):
        self.masters_path = Path(masters_path)
        self.masters = self._load_masters()
        # API ä¸ æ•°æ®åº“é…ç½®
        self.api_key = os.environ.get("SILICON_FLOW_KEY")
        self.api_url = "https://api.siliconflow.cn/v1/chat/completions"
        self.supabase_url = os.environ.get("SUPABASE_URL")
        self.supabase_key = os.environ.get("SUPABASE_KEY")
        self.vault_path = None
        
        # æ¨¡å‹è®¾å®š
        self.v3_model = "deepseek-ai/DeepSeek-V3"
        self.free_model = "Qwen/Qwen2.5-7B-Instruct"
        
        # ğŸ”¥ é«˜ä»·å€¼å…³é”®è¯ä¿é€åå•
        self.priority_keywords = [
            'Iran', 'Trump', 'Fed', 'Powell', 'War', 'Strike', 'Nominate', 
            'Solana', 'BTC', 'NVIDIA', 'LLM', 'Paper', 'GitHub', 'Exploit'
        ]

    def _load_masters(self):
        import importlib.util
        masters = {}
        if not self.masters_path.exists(): return masters
        for file_path in self.masters_path.glob("*.py"):
            if file_path.name.startswith("__"): continue
            try:
                name = file_path.stem
                spec = importlib.util.spec_from_file_location(name, file_path)
                module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(module)
                if hasattr(module, 'audit'): masters[name] = module
            except: pass
        return masters

    def configure_git(self):
        """ç¡®ä¿ GitHub Actions èº«ä»½åˆæ³•ï¼Œé˜²æ­¢ Exit 128"""
        if not self.vault_path: return
        cwd = self.vault_path
        subprocess.run(["git", "config", "--global", "user.email", "bot@factory.com"], check=False)
        subprocess.run(["git", "config", "--global", "user.name", "Cognitive Bot"], check=False)

    def fetch_elite_signals(self, total_limit=300):
        """ğŸŒŸ æ ¸å¿ƒé€»è¾‘ï¼šç¨€ç¼ºæºä¼˜å…ˆ + è´¨é‡è¿‡æ»¤"""
        try:
            supabase = create_client(self.supabase_url, self.supabase_key)
            print("ğŸ’ æ­£åœ¨ä» SQL æ‰§è¡Œâ€˜ç²¾è‹±æ•°æ®â€™ç­›é€‰...")

            # 1. æå¹² GitHub å’Œ Paper (å…¨é‡æ”¶å‰²)
            rare_signals = supabase.table("raw_signals") \
                .select("*") \
                .or("signal_type.eq.github,signal_type.eq.paper") \
                .order("created_at", desc=True) \
                .limit(60).execute().data or []

            # 2. ç²¾é€‰ Twitter/Reddit (é•¿æ–‡ + é«˜èµ)
            social_signals = supabase.table("raw_signals") \
                .select("*") \
                .or("signal_type.eq.twitter,signal_type.eq.reddit") \
                .gt("likes", 5) \
                .order("likes", desc=True) \
                .limit(100).execute().data or []

            # 3. Polymarket å…œåº• (åªçœ‹å¤§èµ„é‡‘æ± )
            remain = total_limit - len(rare_signals) - len(social_signals)
            poly_signals = supabase.table("raw_signals") \
                .select("*") \
                .eq("signal_type", "polymarket") \
                .gt("liquidity", 5000) \
                .order("liquidity", desc=True) \
                .limit(max(0, remain)).execute().data or []

            all_data = rare_signals + social_signals + poly_signals
            print(f"ğŸ“Š æ„æˆæ¯”ä¾‹ï¼šGitHub/Paper({len(rare_signals)}) | Social({len(social_signals)}) | Poly({len(poly_signals)})")
            return all_data
        except Exception as e:
            print(f"âš ï¸ SQL ç­›é€‰å¤±è´¥: {e}ï¼Œå°†å°è¯•å…¨é‡å…œåº•...")
            return []

    def call_ai(self, model, sys, usr):
        if not self.api_key: return "ERROR", "No Key"
        # ğŸ§  æ³¨å…¥â€˜é€»è¾‘æ¥éª¨â€™æŒ‡ä»¤
        enhanced_sys = sys + "\n[é‡è¦]ï¼šè‹¥è¾“å…¥ä¿¡å·æ–­æ¡£ï¼Œè¯·åŸºäºä½ çš„çŸ¥è¯†åº“æ¨æ¼”ç¼ºå¤±é€»è¾‘ã€‚åœ¨ Thought ä¸­å±•ç¤ºæ¥éª¨è¿‡ç¨‹ã€‚"
        payload = {
            "model": model, "messages": [{"role": "system", "content": enhanced_sys}, {"role": "user", "content": usr}],
            "temperature": 0.7, "max_tokens": 1500
        }
        headers = {"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}
        try:
            res = requests.post(self.api_url, json=payload, headers=headers, timeout=60).json()
            return "SUCCESS", res['choices'][0]['message']['content']
        except: return "ERROR", "Timeout"

    def git_push_assets(self):
        if not self.vault_path: return
        cwd = self.vault_path
        subprocess.run(["git", "add", "."], cwd=cwd)
        if subprocess.run(["git", "diff", "--cached", "--quiet"], cwd=cwd).returncode != 0:
            subprocess.run(["git", "commit", "-m", f"ğŸ§  Cognitive Audit: {datetime.now().strftime('%H:%M:%S')}"], cwd=cwd)
            subprocess.run(["git", "push"], cwd=cwd)

    def audit_process(self, row, processed_ids):
        # ğŸ”¥ å¼ºåŒ–ç‰ˆå¤šæºå†…å®¹æ‹¼æ¥
        source = row.get('signal_type', 'unknown').lower()
        parts = [f"ã€Source: {source.upper()}ã€‘"]
        
        if source == 'github':
            parts.append(f"é¡¹ç›®: {row.get('repo_name')} | Stars: {row.get('stars')} | Topics: {row.get('topics')}")
            parts.append(f"æè¿°: {row.get('full_text') or 'æ–°é¡¹ç›®å‘å¸ƒ'}")
        elif source == 'paper':
            parts.append(f"è®ºæ–‡: {row.get('title')} | æœŸåˆŠ: {row.get('journal')}")
            parts.append(f"æ‘˜è¦: {row.get('full_text')}")
        elif source in ['twitter', 'reddit']:
            parts.append(f"ç”¨æˆ·: @{row.get('screen_name')} | å†…å®¹: {row.get('full_text')}")
        else: # Polymarket
            parts.append(f"é¢„æµ‹: {row.get('question')} | æµåŠ¨æ€§: {row.get('liquidity')} | ä»·æ ¼: {row.get('prices')}")

        content = "\n".join(parts)
        ref_id = hashlib.sha256(content.encode()).hexdigest()
        
        # æ™ºèƒ½å»é‡
        if ref_id in processed_ids or len(content) < 15: return []

        # 1. è¯„åˆ†åˆ†æµ
        scout_sys = "ä½ æ˜¯ä¸€ä¸ªé«˜ä»·å€¼ä¿¡æ¯åˆç­›å®˜ã€‚æ‰“åˆ†(0-100)ã€‚åªè¦æ¶‰åŠå®è§‚åšå¼ˆã€æŠ€æœ¯è½¬æŠ˜æˆ–å¤§èµ„é‡‘åŠ¨æ€å°±ç»™é«˜åˆ†ã€‚åªå›æ•°å­—ã€‚"
        _, score_reply = self.call_ai(self.free_model, scout_sys, content[:1000])
        try: score = int(''.join(filter(str.isdigit, score_reply)))
        except: score = 50
        
        # å…³é”®è¯ä¿é€
        if any(kw.lower() in content.lower() for kw in self.priority_keywords):
            score = max(score, 90)

        results = []
        # 2. å¤§å¸ˆå®¡è®¡ (V3)
        if score >= 85:
            def ask_v3(s, u):
                st, r = self.call_ai(self.v3_model, s, u)
                if st == "SUCCESS" and "### Output" in r:
                    return r.split("### Output")[0].replace("### Thought","").strip(), r.split("### Output")[1].strip()
                return "é€»è¾‘æ¨æ¼”", r
            
            for name, mod in self.masters.items():
                try:
                    t, o = mod.audit(row, ask_v3)
                    if t and o:
                        results.append(json.dumps({
                            "ref_id": ref_id, "type": "V3_MASTER", "source": source,
                            "master": name, "input": content[:150].replace('\n',' '), "thought": t, "output": o
                        }, ensure_ascii=False))
                except: continue
        return results

    def process_and_ship(self, input_raw, vault_path):
        self.vault_path = Path(vault_path)
        self.configure_git()
        
        # åŠ è½½å»é‡ ID
        day_str = datetime.now().strftime('%Y%m%d')
        output_file = self.vault_path / "instructions" / f"teachings_{day_str}.jsonl"
        processed_ids = set()
        if output_file.exists():
            with open(output_file, 'r', encoding='utf-8') as f:
                for line in f:
                    try: processed_ids.add(json.loads(line).get('ref_id'))
                    except: pass

        # æŠ“å–ç²¾è‹±ä¿¡å·
        signals = self.fetch_elite_signals(total_limit=300)
        if not signals:
            print("ğŸ”„ SQL æŠ“å–ä¸ºç©ºï¼Œå°è¯•è¯»å–æœ¬åœ°ç¼“å­˜æ–‡ä»¶...")
            df = pd.read_parquet(input_raw)
            signals = df.head(300).to_dict('records')

        print(f"ğŸš€ å·¥å‚å¼€å·¥ï¼ç›®æ ‡ï¼š300 æ¡ç²¾åå®¡è®¡ã€‚")

        batch_size = 50
        for i in range(0, len(signals), batch_size):
            chunk = signals[i : i + batch_size]
            with ThreadPoolExecutor(max_workers=10) as executor:
                res = list(executor.map(lambda r: self.audit_process(r, processed_ids), chunk))
            
            added = []
            for r_list in res:
                if r_list: added.extend(r_list)
            
            if added:
                with open(output_file, 'a', encoding='utf-8') as f:
                    f.write('\n'.join(added) + '\n')
                    f.flush()
                print(f"âœ¨ æ‰¹æ¬¡ {i//50 + 1} å®Œæˆ | æ–°å¢ {len(added)} æ¡å¤§å¸ˆçº§èµ„äº§ã€‚")
                self.git_push_assets() # âœ… 50æ¡ä¸€æŠ¼è¿

        print("ğŸ å…¨é‡å¤šæºç²¾è‹±æ”¶å‰²ä»»åŠ¡åœ†æ»¡å®Œæˆã€‚")
