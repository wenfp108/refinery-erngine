import pandas as pd
import hashlib, json, os, requests, subprocess, time, sys
from pathlib import Path
from datetime import datetime, timezone, timedelta
from concurrent.futures import ThreadPoolExecutor
from supabase import create_client
import importlib.util

class UniversalFactory:
    def __init__(self, masters_path="masters"):
        self.masters_path = Path(masters_path)
        self.masters = self._load_masters()
        self.api_key = os.environ.get("SILICON_FLOW_KEY") 
        self.api_url = "https://api.siliconflow.cn/v1/chat/completions"
        self.supabase_url = os.environ.get("SUPABASE_URL")
        self.supabase_key = os.environ.get("SUPABASE_KEY")
        self.v3_model = "deepseek-ai/DeepSeek-V3"
        self.vault_path = None
        self.memory = {} 

    def _load_masters(self):
        masters = {}
        if not self.masters_path.exists(): return masters
        for file_path in self.masters_path.glob("*.py"):
            if file_path.name.startswith("__"): continue
            try:
                name = file_path.stem
                spec = importlib.util.spec_from_file_location(name, file_path)
                module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(module)
                if hasattr(module, 'audit'): masters[name] = module
                print(f"âœ… å·²åŠ è½½ Master: {name}")
            except: pass
        return masters

    def build_day_memory(self, vault_path):
        """ğŸ§  è·¨æ—¶åŒºè®°å¿†åŒæ­¥ï¼šé”å®šä»Šæ—¥å·²å®¡è®¡çš„å“ˆå¸Œï¼Œçœé’±æ ¸å¿ƒ"""
        day_str = datetime.now().strftime('%Y%m%d')
        instructions_dir = vault_path / "instructions"
        if not instructions_dir.exists(): return set()
        
        day_processed_ids = set()
        print(f"ğŸ§ æ­£åœ¨åŠ è½½ä»Šæ—¥å…¨å¤©ï¼ˆ2å°æ—¶æ­¥è¿›ï¼‰è®°å¿†...")
        for f in instructions_dir.glob(f"teachings_{day_str}_*.jsonl"):
            try:
                with open(f, 'r', encoding='utf-8') as f_in:
                    for line in f_in:
                        try:
                            data = json.loads(line)
                            tid, m, rid = data.get('topic_id'), data.get('master'), data.get('ref_id')
                            if tid and m:
                                if tid not in self.memory: self.memory[tid] = {}
                                self.memory[tid][m] = data.get('output', "")
                            if rid: day_processed_ids.add(rid)
                        except: continue
            except: pass
        print(f"âœ… è®°å¿†æ„å»ºï¼šé”å®š {len(day_processed_ids)} ä¸ªå†å²å“ˆå¸Œ")
        return day_processed_ids

    def fetch_elite_signals(self):
        """ğŸŒŸ ä¸¥æ ¼ä¿ç•™ä½ çš„åŸè£…æƒé‡ 50/60/30/80"""
        try:
            supabase = create_client(self.supabase_url, self.supabase_key)
            print("ğŸ’ å¯åŠ¨ 2 å°æ—¶ä¸€åº¦ç²¾é”ç­›é€‰...")

            # === 1. GitHub ä¿¡å·ç‹¬ç«‹å¤„ç† (ä¿åº• 20 æ¡) ===
            print("ğŸ’ æ­£åœ¨è·å– GitHub ä¿¡å·...")
            # è¿™é‡Œçš„ limit æ”¹æˆäº† 100ï¼Œå¤šæŠ“ç‚¹æ›´ä¿é™©
            github_raw = supabase.table("raw_signals").select("*").eq("signal_type", "github").order("created_at", desc=True).limit(100).execute().data or []
            
            unique_github = {}
            for r in github_raw:
                # GitHub ä¸“å±å»é‡é”®ï¼šrepo_name
                name = r.get('repo_name')
                if name and name not in unique_github:
                    unique_github[name] = r
            
            github_picks = list(unique_github.values())[:20]  # ç¨³æ‹¿ 20 æ¡
            print(f"âœ… GitHub ç‹¬ç«‹å¤„ç†å®Œæˆï¼šè· {len(github_picks)} æ¡")

            # === 2. Paper ä¿¡å·ç‹¬ç«‹å¤„ç† (ä¿åº• 30 æ¡) ===
            print("ğŸ’ æ­£åœ¨è·å– Paper ä¿¡å·...")
            # è¿™é‡Œçš„ limit ä¹Ÿæ”¹æˆäº† 100
            paper_raw = supabase.table("raw_signals").select("*").eq("signal_type", "papers").order("created_at", desc=True).limit(100).execute().data or []
            
            unique_paper = {}
            for r in paper_raw:
                # Paper ä¸“å±å»é‡é”®ï¼štitle (å¢åŠ é˜²å¾¡é€»è¾‘ï¼Œé˜²æ­¢å› ä¸ºæ²¡æ ‡é¢˜è¢«æ‰”æ‰)
                title = r.get('title') or r.get('headline')
                
                # å¦‚æœæ²¡æ ‡é¢˜ï¼Œå¼ºè¡Œæˆªå–æ­£æ–‡å‰30å­—å½“æ ‡é¢˜ï¼Œç¡®ä¿æ•°æ®ä¸ä¸¢å¤±
                if not title and r.get('full_text'):
                    title = r.get('full_text')[:30]

                if title and title not in unique_paper:
                    # ğŸš¨ å…³é”®ä¿®å¤ï¼šæŠŠæ‰¾åˆ°çš„æ ‡é¢˜å†™å› rï¼Œé˜²æ­¢åé¢ç”Ÿæˆ Prompt æ—¶ title è¿˜æ˜¯ None
                    r['title'] = title 
                    unique_paper[title] = r
            
            paper_picks = list(unique_paper.values())[:30]  # ç¨³æ‹¿ 30 æ¡
            print(f"âœ… Paper ç‹¬ç«‹å¤„ç†å®Œæˆï¼šè· {len(paper_picks)} æ¡")

            # === 3. Twitter (VIP æƒé‡) - ä¿æŒåŸæ · ===
            print("ğŸ’ æ­£åœ¨è·å– Twitter ä¿¡å·...")
            tw_raw = supabase.table("raw_signals").select("*").eq("signal_type", "twitter").order("created_at", desc=True).limit(500).execute().data or []
            vip_list = ['Karpathy', 'Musk', 'Vitalik', 'LeCun', 'Dalio', 'Naval', 'Sama', 'PaulG']
            def score_twitter(row):
                rt, bm, like = row.get('retweets',0), row.get('bookmarks',0), row.get('likes',0)
                user = str(row.get('user_name', '')).lower()
                score = (rt * 5) + (bm * 10) + like
                if any(v.lower() in user for v in vip_list):
                    score += 10000 if (rt > 10 or like > 50) else 500
                return score
            for r in tw_raw: r['_rank'] = score_twitter(r)
            tw_picks = sorted(tw_raw, key=lambda x:x['_rank'], reverse=True)[:60]
            print(f"âœ… Twitter å¤„ç†å®Œæˆï¼šè· {len(tw_picks)} æ¡")

            # === 4. Reddit (Vibe æƒé‡) - ä¿æŒåŸæ · ===
            print("ğŸ’ æ­£åœ¨è·å– Reddit ä¿¡å·...")
            rd_raw = supabase.table("raw_signals").select("*").eq("signal_type", "reddit").order("created_at", desc=True).limit(500).execute().data or []
            unique_rd = {r.get('url'): r for r in rd_raw if r.get('url')}
            def score_reddit(row): return (row.get('score') or 0) * (1 + abs(float(row.get('vibe') or 0)))
            rd_picks = sorted(unique_rd.values(), key=score_reddit, reverse=True)[:30]
            print(f"âœ… Reddit å¤„ç†å®Œæˆï¼šè· {len(rd_picks)} æ¡")

            # === 5. Polymarket (Tail_Risk æƒé‡) - ä¿æŒåŸæ · ===
            print("ğŸ’ æ­£åœ¨è·å– Polymarket ä¿¡å·...")
            poly_raw = supabase.table("raw_signals").select("*").eq("signal_type", "polymarket").order("created_at", desc=True).limit(800).execute().data or []
            unique_poly = {}
            for p in poly_raw:
                raw = p.get('raw_json')
                if isinstance(raw, str):
                    try: raw = json.loads(raw)
                    except: raw = {}
                p['_parsed'] = raw
                slug = p.get('slug') or raw.get('slug')
                if slug:
                    curr_liq = float(p.get('liquidity') or 0)
                    if slug not in unique_poly or curr_liq > float(unique_poly[slug].get('liquidity',0)):
                        unique_poly[slug] = p
            def score_poly(row):
                raw, liq = row['_parsed'], float(row.get('liquidity') or 0)
                if 'TAIL_RISK' in raw.get('strategy_tags', []): return 10000000 + liq
                if any(x in str(row.get('category','')).upper() for x in ['ECONOMY', 'TECH']): return 5000000 + liq
                return 1000000 + liq
            poly_picks = sorted(unique_poly.values(), key=score_poly, reverse=True)[:80]
            print(f"âœ… Polymarket å¤„ç†å®Œæˆï¼šè· {len(poly_picks)} æ¡")

            return github_picks + paper_picks + tw_picks + rd_picks + poly_picks
        except Exception as e:
            print(f"âš ï¸ ç­›é€‰å¼‚å¸¸: {e}"); return []

    def audit_process(self, row, processed_ids):
        topic_id = row.get('url') or row.get('slug') or row.get('repo_name') or "unknown"
        source = row.get('signal_type', 'unknown').lower()
        
        # ä¸¥æ ¼å¯¹é½æ ¼å¼
        parts = [f"ã€Source: {source.upper()}ã€‘"]
        if source == 'github':
            parts.append(f"é¡¹ç›®: {row.get('repo_name')} | Stars: {row.get('stars')} | Topics: {row.get('topics')}")
            parts.append(f"æè¿°: {row.get('full_text') or 'æ–°é¡¹ç›®å‘å¸ƒ'} | Link: {row.get('url')}")
        elif source == 'papers':
            parts.append(f"è®ºæ–‡: {row.get('title')} | æœŸåˆŠ: {row.get('journal')}")
            parts.append(f"å¼•ç”¨: {row.get('citations')} | æ‘˜è¦: {row.get('full_text')}")
        elif source in ['twitter', 'reddit']:
            parts.append(f"ç”¨æˆ·: {row.get('user_name') or row.get('subreddit')} | Score: {row.get('_rank',0)}")
            parts.append(f"å†…å®¹: {row.get('full_text') or row.get('title')}")
        else: # Polymarket
            raw = row.get('_parsed') or row.get('raw_json') or {}
            parts.append(f"é¢„æµ‹: {row.get('title')} | é—®é¢˜: {row.get('question')}")
            parts.append(f"ä»·æ ¼: {row.get('prices') or raw.get('outcome_prices')} | æµåŠ¨æ€§: ${raw.get('liquidity')}")

        content = "\n".join(parts)
        ref_id = hashlib.sha256(content.encode()).hexdigest()
        
        # æ ¸å¿ƒå»é‡ï¼šå¦‚æœä»Šå¤©å®¡è¿‡ï¼Œç›´æ¥è·³è¿‡ï¼Œä¸èŠ± API é’±
        if ref_id in processed_ids: return []

        results = []
        def ask_v3(s, u):
            st, r = self.call_ai(self.v3_model, s, u)
            if st == "SUCCESS" and "### Output" in r:
                return r.split("### Output")[0].replace("### Thought","").strip(), r.split("### Output")[1].strip()
            return "Audit", r

        for name, mod in self.masters.items():
            prev_opinion = self.memory.get(topic_id, {}).get(name)
            drift_context = f"\n\n[å†å²è®°å¿†]ï¼šæ­¤å‰è§‚ç‚¹ï¼š'{prev_opinion}'ã€‚æ•°æ®å˜åŠ¨è‹¥è§¦å‘é€»è¾‘åè½¬ï¼Œè¯·åœ¨ Output å¼€å¤´æ ‡è®° [DRIFT_DETECTED]ã€‚" if prev_opinion else ""
            try:
                if hasattr(mod, 'audit'):
                    row['_drift_context'] = drift_context
                    row['full_text_formatted'] = content
                    t, o = mod.audit(row, ask_v3)
                    if t and o:
                        results.append(json.dumps({
                            "ref_id": ref_id, "topic_id": topic_id, "master": name,
                            "drift": "[DRIFT_DETECTED]" in o,
                            "source": source, "thought": t, "output": o
                        }, ensure_ascii=False))
            except: continue
        return results

    def process_and_ship(self, vault_path="vault"):
        self.vault_path = Path(vault_path)
        (self.vault_path / "instructions").mkdir(parents=True, exist_ok=True)
        
        # 1. åŠ è½½ä»Šæ—¥å…¨å¤©å»é‡ ID
        processed_ids = self.build_day_memory(self.vault_path)
        
        now = datetime.now()
        day_str = now.strftime('%Y%m%d')
        hour_str = now.strftime('%H')
        output_file = self.vault_path / "instructions" / f"teachings_{day_str}_{hour_str}.jsonl"

        # 2. ç­›é€‰
        signals = self.fetch_elite_signals()
        if not signals: return

        # 3. å®¡è®¡å¹¶å®æ—¶é”å®š ID
        batch_size = 50
        for i in range(0, len(signals), batch_size):
            chunk = signals[i : i + batch_size]
            with ThreadPoolExecutor(max_workers=20) as executor:
                res = list(executor.map(lambda r: self.audit_process(r, processed_ids), chunk))
            
            added = []
            for r_list in res:
                if r_list:
                    added.extend(r_list)
                    # å®æ—¶å­˜å…¥ï¼Œé˜²æ­¢åŒä¸€æ‰¹æ¬¡å†…ç”±äº Supabase å»¶è¿Ÿå¯¼è‡´çš„é‡å¤
                    for r_json in r_list: processed_ids.add(json.loads(r_json).get('ref_id'))
            
            if added:
                with open(output_file, 'a', encoding='utf-8') as f:
                    f.write('\n'.join(added) + '\n')
                self.git_push_assets()

    def call_ai(self, model, sys_prompt, usr_prompt):
        headers = {"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}
        payload = {"model": model, "messages": [{"role": "system", "content": sys_prompt}, {"role": "user", "content": usr_prompt}], "temperature": 0.7}
        try:
            res = requests.post(self.api_url, json=payload, headers=headers, timeout=60).json()
            return "SUCCESS", res['choices'][0]['message']['content']
        except: return "ERROR", "AI_FAIL"

    def git_push_assets(self):
        """é˜²å¾¡å‹æ¨é€ï¼šè§£å†³èº«ä»½æœªçŸ¥ã€æœªæäº¤æ›´æ”¹ä»¥åŠè¿œç¨‹æ‹’ç»é—®é¢˜"""
        if not self.vault_path: return
        cwd = self.vault_path
        
        # === ğŸ›¡ï¸ æ–°å¢ï¼šè‡ªæ„ˆé€»è¾‘ ===
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨åƒµå°¸ rebase é”ï¼Œå¦‚æœæœ‰ï¼Œå…ˆæ€æ‰
        rebase_dir = cwd / ".git" / "rebase-merge"
        if rebase_dir.exists():
            print("ğŸš‘ æ£€æµ‹åˆ°åƒµå°¸ Rebase é”ï¼Œæ­£åœ¨æ‰§è¡Œæˆ˜åœ°æ€¥æ•‘...")
            subprocess.run(["git", "rebase", "--abort"], cwd=cwd)
            if rebase_dir.exists(): # å¦‚æœ abort å¤±è´¥ï¼Œç›´æ¥ç‰©ç†åˆ é™¤
                import shutil
                shutil.rmtree(rebase_dir)
        # =======================

        # 1. å¼ºåˆ¶æ³¨å…¥èº«ä»½
        subprocess.run(["git", "config", "user.email", "bot@factory.com"], cwd=cwd)
        subprocess.run(["git", "config", "user.name", "Cognitive Bot"], cwd=cwd)
        # è§£å†³ pull æ—¶çš„ rebase ç­–ç•¥è­¦å‘Š
        subprocess.run(["git", "config", "pull.rebase", "true"], cwd=cwd)

        # 2. ã€é¡ºåºè°ƒæ•´ã€‘å…ˆ add å’Œ commitï¼ŒæŠŠä½ çš„ 1000 å¤šæ¡æ•°æ®å­˜è¿›æœ¬åœ°ä»“åº“
        subprocess.run(["git", "add", "."], cwd=cwd)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸œè¥¿å¯ä»¥ commit
        diff_status = subprocess.run(["git", "diff", "--cached", "--quiet"], cwd=cwd)
        if diff_status.returncode == 0:
            print("ğŸ’¤ æ²¡æœ‰å‘ç°æ–°èµ„äº§ï¼Œè·³è¿‡åŒæ­¥ã€‚")
            return

        # 3. æ‰§è¡Œ Commit
        commit_msg = f"ğŸ§  Cognitive Audit: {datetime.now().strftime('%H:%M:%S')}"
        subprocess.run(["git", "commit", "-m", commit_msg], cwd=cwd)

        # 4. ã€åŒæ­¥è¿œç¨‹ã€‘æ­¤æ—¶å† pull --rebaseï¼ŒGit å°±èƒ½é¡ºç•…åœ°æŠŠè¿œç¨‹æ”¹åŠ¨æ¥åœ¨ä½ çš„ commit ä¹‹å
        print("ğŸ”„ æ­£åœ¨é€šè¿‡ rebase åŒæ­¥è¿œç¨‹ä»“åº“...")
        subprocess.run(["git", "pull", "origin", "main", "--rebase"], cwd=cwd)

        # 5. æœ€ç»ˆæ¨é€
        push_res = subprocess.run(["git", "push", "origin", "main"], cwd=cwd, capture_output=True, text=True)
        
        if push_res.returncode == 0:
            print("ğŸš€ è®¤çŸ¥èµ„äº§å·²æˆåŠŸåŒæ­¥è‡³ä¸­å¤®é“¶è¡Œã€‚")
        else:
            print(f"âŒ æœ€ç»ˆæ¨é€å¤±è´¥: {push_res.stderr}")

if __name__ == "__main__":
    factory = UniversalFactory()
    factory.process_and_ship()
